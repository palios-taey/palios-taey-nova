{"data_id": "67bf1053-9830-8000-99fd-ae619e1a1ed0-c88a20f4-508f-4d3c-bffa-30970961d8bd", "content": ["Below is a revised version of your **main.py** that includes the Gemini Pro API integration. I\u2019ve imported the new Gemini API module and added a new endpoint function, **execute_gemini**, to test the Gemini integration. This new function instantiates the Gemini API client and sends a prompt from the incoming request.\n\n> **Note:** Make sure you have already created and updated the **gemini_integration.py** module (as guided earlier) and that its **API endpoint placeholder** is replaced with your actual Gemini Pro API URL.\n\n---\n\n```python\nimport os\nfrom datetime import datetime\nfrom google.cloud import firestore\nimport openai\nfrom google.cloud import aiplatform\nimport json\nimport uuid\nimport http.server\nimport socketserver\nfrom http import HTTPStatus\nimport logging\n\n# Import the Gemini Pro API integration module\nfrom gemini_integration import GeminiProAPI\n\n# Initialize logging\nlogging.basicConfig(level=logging.DEBUG,  # Set default logging level to DEBUG (for MVP)\n                    format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Instantiate the Gemini Pro API client globally.\ntry:\n    gemini_api = GeminiProAPI()\n    logging.info(\"\u2705 Gemini Pro API client initialized successfully.\")\nexcept Exception as e:\n    logging.error(\"\u274c Gemini Pro API client initialization failed: \" + str(e))\n    gemini_api = None  # This ensures the rest of the system can still run\n\nclass AISystem:\n    def __init__(self):\n        \"\"\"Initialize the AI system with all necessary connections and logging\"\"\"\n        logging.info(\"Initializing AI System...\")\n        self.db = firestore.Client(project='ai-execution-system')\n        self.setup_api_keys()\n        self.setup_collections()\n        self.register_ai_capabilities()\n        logging.info(\"\u2705 AI System Initialized.\")\n\n    def register_ai_capabilities(self):\n        \"\"\"Register available AI models and their capabilities\"\"\"\n        logging.debug(\"Registering AI capabilities...\")\n        self.ai_capabilities = {\n            'gpt-4-turbo-preview': {\n                'provider': 'openai',\n                'capabilities': ['text_generation', 'analysis', 'planning', 'reasoning'],\n                'context_length': 128000,\n                'task_types': ['complex_reasoning', 'creative', 'analytical', 'planning']\n            },\n            'gpt-3.5-turbo': {\n                'provider': 'openai',\n                'capabilities': ['text_generation', 'analysis'],\n                'context_length': 16000,\n                'task_types': ['simple_queries', 'clarification', 'validation']\n            },\n            'vertex-text': {\n                'provider': 'vertex',\n                'capabilities': ['text_generation', 'structured_analysis'],\n                'task_types': ['data_processing', 'pattern_recognition']\n            }\n        }\n        logging.debug(\"AI capabilities registered.\")\n\n    def setup_api_keys(self):\n        \"\"\"Retrieve and set up API keys from Firestore with error handling and logging\"\"\"\n        try:\n            logging.debug(\"Attempting to retrieve API keys from Firestore...\")\n            keys_ref = self.db.collection('keys')\n            docs = keys_ref.stream()\n            for doc in docs:\n                keys = doc.to_dict()\n                self.openai_key = keys.get('OpenAI')\n                self.vertex_key = keys.get('Vertex AI')\n                break\n\n            openai.api_key = self.openai_key\n            aiplatform.init(project='ai-execution-system', location='us-central1')\n            logging.info(\"\u2705 API keys configured successfully\")\n        except Exception as e:\n            logging.error(\"\u274c ERROR in setup_api_keys() method!\")\n            logging.error(\"\u274c Exception Type: \" + str(type(e)))\n            logging.error(\"\u274c Error Arguments: \" + str(e.args))\n            logging.error(\"\u274c Full Exception Details: \" + str(e))\n            logging.error(\"\u274c Double-check Firestore setup, credentials, and network connection.\")\n            raise\n\n    def setup_collections(self):\n        \"\"\"Set up or verify Firestore collections with error handling and logging\"\"\"\n        try:\n            logging.debug(\"Attempting to set up Firestore collections...\")\n            self.executions_ref = self.db.collection('executions')\n            self.tasks_ref = self.db.collection('tasks')\n            self.results_ref = self.db.collection('results')\n\n            test_id = f\"test_{uuid.uuid4()}\"\n            self.executions_ref.document(test_id).set({\n                'timestamp': datetime.now(),\n                'type': 'system_init',\n                'status': 'success'\n            })\n            self.executions_ref.document(test_id).delete()\n\n            logging.info(\"\u2705 Firestore collections configured successfully\")\n        except Exception as e:\n            logging.error(\"\u274c ERROR in setup_collections() method!\")\n            logging.error(\"\u274c Exception Type: \" + str(type(e)))\n            logging.error(\"\u274c Error Arguments: \" + str(e.args))\n            logging.error(\"\u274c Full Exception Details: \" + str(e))\n            logging.error(\"\u274c Double-check Firestore setup, credentials, and permissions.\")\n            raise\n\n    def create_task(self, task_type, task_data):\n        \"\"\"Create a new task in the system with error handling and logging\"\"\"\n        task_id = str(uuid.uuid4())\n        logging.debug(f\"Creating task of type '{task_type}' with ID: {task_id}\")\n        try:\n            task_ref = self.tasks_ref.document(task_id)\n            task_ref.set({\n                'task_id': task_id,\n                'type': task_type,\n                'data': task_data,\n                'status': 'pending',\n                'created_at': datetime.now(),\n                'updated_at': datetime.now()\n            })\n            logging.info(f\"\u2705 Task Created. Task ID: {task_id}\")\n            return task_id\n        except Exception as e:\n            logging.error(f\"\u274c Error creating task (Task ID: {task_id}, Type: {task_type}): {str(e)}\")\n            raise\n\n    def create_collaborative_task(self, task_description: str) -> str:\n        \"\"\"Create a collaborative task with enhanced logging and error handling\"\"\"\n        task_id = str(uuid.uuid4())\n        logging.debug(f\"Creating collaborative task. Task ID: {task_id}, Description: '{task_description}'\")\n        try:\n            logging.debug(f\"create_collaborative_task - START - Task ID: {task_id}\")\n            subtasks = self._decompose_task(task_description)\n            logging.debug(f\"create_collaborative_task - subtasks decomposed. Count: {len(subtasks)}\")\n\n            task_ref = self.tasks_ref.document(task_id)\n            task_ref.set({\n                'task_id': task_id,\n                'type': 'collaborative',\n                'description': task_description,\n                'subtasks': subtasks,\n                'status': 'pending',\n                'created_at': datetime.now(),\n                'updated_at': datetime.now(),\n                'results': []\n            })\n            logging.debug(f\"create_collaborative_task - task_ref.set completed for Task ID: {task_id}\")\n            logging.info(f\"\u2705 Collaborative Task Created. Task ID: {task_id}\")\n            logging.debug(f\"create_collaborative_task - END - Task ID: {task_id}\")\n            return task_id\n        except Exception as e:\n            logging.error(f\"\u274c Error creating collaborative task (Task ID: {task_id}): {str(e)}\")\n            raise\n\n    def _decompose_task(self, task_description: str) -> list:\n        \"\"\"Break down a complex task into subtasks with enhanced logging and error handling\"\"\"\n        logging.debug(\"_decompose_task - START\")\n        try:\n            logging.debug(\"_decompose_task - Calling openai.chat.completions.create...\")\n            response = openai.chat.completions.create(\n                model=\"gpt-4-turbo-preview\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a task decomposition AI. Return only raw JSON with no markdown formatting or other text.\"},\n                    {\"role\": \"user\", \"content\": f\"\"\"\n                                                 Break this task into subtasks. For each subtask, provide a JSON object with these fields:\n                                                 - \"objective\": specific goal of the subtask\n                                                 - \"capabilities\": array of required capabilities from [{', '.join(self.ai_capabilities['gpt-4-turbo-preview']['capabilities'])}]\n                                                 - \"output_format\": expected format of the result\n\n                                                 Return ONLY a JSON array of these objects with no additional formatting or explanation.\n\n                                                 Task: {task_description}\n                                                 \"\"\"}\n                ]\n            )\n            logging.debug(\"_decompose_task - openai.chat.completions.create completed\")\n            content = response.choices[0].message.content\n            logging.debug(\"_decompose_task - response.choices[0].message.content retrieved\")\n            if '```' in content:\n                content = content.split('```')[1]\n                if content.startswith('json'):\n                    content = content[4:]\n                content = content.strip()\n            logging.debug(\"_decompose_task - markdown cleanup completed (if any)\")\n            subtasks_raw = json.loads(content)\n            logging.debug(\"_decompose_task - json.loads(content) completed\")\n            subtasks = []\n            for subtask in subtasks_raw:\n                subtask.update({\n                    'subtask_id': str(uuid.uuid4()),\n                    'status': 'pending',\n                    'assigned_model': self._select_model_for_subtask(subtask),\n                    'result': None\n                })\n                subtasks.append(subtask)\n            logging.debug(\"_decompose_task - subtasks metadata added\")\n            logging.debug(\"_decompose_task - END - Returning subtasks\")\n            return subtasks\n\n        except json.JSONDecodeError as json_e:\n            logging.error(f\"\u274c JSONDecodeError in _decompose_task: Invalid JSON format from OpenAI. Error: {json_e}\")\n            if 'content' in locals():\n                logging.error(f\"Raw content received from OpenAI: {content}\")\n            raise\n        except Exception as e:\n            logging.error(f\"\u274c Error decomposing task: {str(e)}\")\n            if 'response' in locals() and hasattr(response, 'choices') and response.choices and response.choices[0].message.content:\n                logging.error(f\"Raw response content from OpenAI: {response.choices[0].message.content}\")\n            if 'content' in locals():\n                logging.error(f\"Cleaned content: {content}\")\n            raise\n\n    def _select_model_for_subtask(self, subtask: dict) -> str:\n        \"\"\"Select the most appropriate model for a subtask based on required capabilities\"\"\"\n        required_capabilities = subtask.get('capabilities', [])\n        logging.debug(f\"_select_model_for_subtask - Selecting model for subtask with required capabilities: {required_capabilities}\")\n        model_scores = {}\n        for model, info in self.ai_capabilities.items():\n            score = sum(1 for cap in required_capabilities if cap in info['capabilities'])\n            model_scores[model] = score\n        logging.debug(f\"_select_model_for_subtask - Model scores: {model_scores}\")\n        best_model = max(model_scores.items(), key=lambda x: x[1])[0]\n        logging.debug(f\"_select_model_for_subtask - Best model selected: {best_model}\")\n        return best_model\n\n    def execute_task(self, request):\n        \"\"\"HTTP Cloud Function to execute AI task based on task description.\"\"\"\n        try:\n            request_json = request.get_json()\n            if request_json and 'task_description' in request_json:\n                task_description = request_json['task_description']\n                # Initialize Firestore to get API keys\n                db = firestore.Client(project='ai-execution-system')\n                keys_ref = db.collection('keys')\n                docs = keys_ref.stream()\n                openai_key = None\n                for doc in docs:\n                    keys = doc.to_dict()\n                    openai_key = keys.get('OpenAI')\n                    break\n                if not openai_key:\n                    return json.dumps({\"error\": \"OpenAI API key not found in Firestore\"}), 500, {'Content-Type': 'application/json'}\n                openai.api_key = openai_key\n                response = openai.chat.completions.create(\n                    model=\"gpt-3.5-turbo\",\n                    messages=[{\"role\": \"user\", \"content\": task_description}]\n                )\n                ai_response = response.choices[0].message.content\n                response_data = {\n                    \"task_description\": task_description,\n                    \"ai_response\": ai_response,\n                    \"status\": \"task_executed_with_ai\",\n                    \"provider\": \"openai\"\n                }\n                return json.dumps(response_data), 200, {'Content-Type': 'application/json'}\n            else:\n                error_message = \"Error: 'task_description' not found in request JSON body.\"\n                logging.error(error_message)\n                return json.dumps({\"error\": error_message}), 400, {'Content-Type': 'application/json'}\n        except Exception as e:\n            error_message = f\"Exception in execute_task: {str(e)}\"\n            logging.error(error_message)\n            return json.dumps({\"error\": error_message}), 500, {'Content-Type': 'application/json'}\n\ndef execute_gemini(request):\n    \"\"\"HTTP Cloud Function to execute a test Gemini prompt using GeminiProAPI.\"\"\"\n    try:\n        request_json = request.get_json()\n        if request_json and 'prompt' in request_json:\n            prompt_text = request_json['prompt']\n        else:\n            prompt_text = \"Hello, Gemini Pro!\"\n        if gemini_api is None:\n            raise Exception(\"Gemini Pro API client not initialized.\")\n        api_response = gemini_api.send_prompt(prompt_text)\n        response_data = {\n            \"prompt\": prompt_text,\n            \"gemini_response\": api_response,\n            \"status\": \"success\"\n        }\n        return json.dumps(response_data), 200, {'Content-Type': 'application/json'}\n    except Exception as e:\n        error_message = f\"Exception in execute_gemini: {str(e)}\"\n        logging.error(error_message)\n        return json.dumps({\"error\": error_message}), 500, {'Content-Type': 'application/json'}\n\ndef serve_health_check(request):\n    \"\"\"HTTP Cloud Function for health check.\"\"\"\n    return {\"status\": \"healthy from Cloud Function\"}\n\nif __name__ == '__main__':\n    PORT = 8080\n    class HealthCheckHandler(http.server.SimpleHTTPRequestHandler):\n        def do_GET(self):\n            if self.path == '/health':\n                self.send_response(HTTPStatus.OK)\n                self.send_header('Content-type', 'application/json')\n                self.end_headers()\n                response_content = json.dumps({\"status\": \"healthy\"})\n                self.wfile.write(response_content.encode())\n            else:\n                super().do_GET()\n    with socketserver.TCPServer((\"\", PORT), HealthCheckHandler) as httpd:\n        print(f\"\u2705 Serving health check on port {PORT} with enhanced logging (local)...\")\n        httpd.serve_forever()\n```\n\n---\n\n### **Explanation:**\n\n- **Gemini Pro API Import & Instantiation:**  \n  The file now imports **GeminiProAPI** from **gemini_integration.py** and attempts to initialize a global **gemini_api** object. If initialization fails, an error is logged and the variable is set to **None**.\n\n- **New Endpoint `execute_gemini`:**  \n  This function is structured similarly to **execute_task** but uses **gemini_api.send_prompt** to send a prompt and return the Gemini API\u2019s response.\n\n- **Preserved Functionality:**  \n  All existing functionality in **main.py** (Firestore, OpenAI integration, health check, etc.) remains unchanged.\n\nThis version of **main.py** should help reduce errors by providing a complete integration with Gemini Pro API in one file. Let me know if you have any further questions or need additional guidance before proceeding to testing!"], "source": "ChatGPT", "timestamp": 1740579431.685967, "data_type": "conversation", "metadata": {"author": "assistant", "original_conversation_id": "67bf1053-9830-8000-99fd-ae619e1a1ed0"}}