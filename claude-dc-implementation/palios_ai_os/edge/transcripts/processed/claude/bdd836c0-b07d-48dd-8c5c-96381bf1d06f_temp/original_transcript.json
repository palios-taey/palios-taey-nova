{
  "data_id": "bdd836c0-b07d-48dd-8c5c-96381bf1d06f",
  "content": [
    " Let me continue with the transcript_processor.py file:\n\n```python\n                \"percentages\": {},\n                \"completion_rate\": 0\n            }\n    \n    def _calculate_communication_metrics(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Calculate communication metrics\n        \n        Args:\n            messages: List of messages\n            \n        Returns:\n            Metrics\n        \"\"\"\n        try:\n            # Calculate basic metrics\n            total_messages = len(messages)\n            \n            if total_messages == 0:\n                return {}\n            \n            # Group by sender\n            senders = {}\n            \n            for message in messages:\n                sender = message.get(\"metadata\", {}).get(\"sender_id\", \"unknown\")\n                \n                if sender not in senders:\n                    senders[sender] = []\n                \n                senders[sender].append(message)\n            \n            # Calculate tokens per message (estimated)\n            total_tokens = 0\n            \n            for message in messages:\n                content = message.get(\"content\", \"\")\n                \n                if isinstance(content, str):\n                    # Rough estimation - 4 chars per token\n                    tokens = len(content) / 4\n                elif isinstance(content, dict):\n                    # Rough estimation for structured content\n                    content_str = json.dumps(content)\n                    tokens = len(content_str) / 4\n                else:\n                    tokens = 0\n                \n                total_tokens += tokens\n            \n            avg_tokens_per_message = total_tokens / total_messages if total_messages > 0 else 0\n            \n            # Calculate response times (if timestamp available)\n            response_times = []\n            \n            # Sort messages by timestamp\n            try:\n                sorted_messages = sorted(\n                    messages, \n                    key=lambda m: datetime.fromisoformat(m.get(\"metadata\", {}).get(\"timestamp\", \"\")) \n                    if isinstance(m.get(\"metadata\", {}).get(\"timestamp\", \"\"), str) else 0\n                )\n                \n                for i in range(1, len(sorted_messages)):\n                    curr_msg = sorted_messages[i]\n                    prev_msg = sorted_messages[i-1]\n                    \n                    curr_time_str = curr_msg.get(\"metadata\", {}).get(\"timestamp\")\n                    prev_time_str = prev_msg.get(\"metadata\", {}).get(\"timestamp\")\n                    \n                    if curr_time_str and prev_time_str:\n                        try:\n                            # Convert to datetime if they're strings\n                            curr_time = curr_time_str if isinstance(curr_time_str, datetime) else datetime.fromisoformat(curr_time_str)\n                            prev_time = prev_time_str if isinstance(prev_time_str, datetime) else datetime.fromisoformat(prev_time_str)\n                            \n                            # Calculate time difference in seconds\n                            time_diff = (curr_time - prev_time).total_seconds()\n                            if time_diff > 0:  # Only add positive time differences\n                                response_times.append(time_diff)\n                        except (ValueError, TypeError) as e:\n                            logger.debug(f\"Error parsing timestamps: {e}\")\n            except Exception as e:\n                logger.warning(f\"Error calculating response times: {e}\")\n            \n            avg_response_time = sum(response_times) / len(response_times) if response_times else 0\n            \n            # Prepare metrics\n            metrics = {\n                \"message_count\": total_messages,\n                \"unique_senders\": len(senders),\n                \"avg_tokens_per_message\": avg_tokens_per_message,\n                \"avg_response_time_seconds\": avg_response_time,\n                \"sender_distribution\": {sender: len(msgs) for sender, msgs in senders.items()}\n            }\n            \n            return metrics\n        except Exception as e:\n            logger.error(f\"Error calculating communication metrics: {str(e)}\")\n            return {\n                \"message_count\": len(messages),\n                \"error\": str(e)\n            }\n    \n    def get_status(self) -> Dict[str, Any]:\n        \"\"\"\n        Get status information about the transcript processor\n        \n        Returns:\n            Status information\n        \"\"\"\n        status = {\n            \"status\": \"active\",\n            \"mode\": \"mock\" if self.use_mock else \"normal\",\n            \"memory_system_available\": self.memory is not None,\n            \"confidence_threshold\": self.confidence_threshold\n        }\n        \n        # Add message counts if in mock mode\n        if self.use_mock:\n            status[\"transcript_count\"] = len(getattr(self, '_transcripts', {}))\n            status[\"message_count\"] = len(getattr(self, '_messages', {}))\n        \n        return status\n\n\n# Convenience function to create transcript processor\ndef create_transcript_processor(\n    memory_system=None, \n    project_id=None, \n    collection_prefix=\"\",\n    use_mock=False\n) -> TranscriptProcessor:\n    \"\"\"\n    Create a transcript processor instance with robust error handling\n    \n    Args:\n        memory_system: Optional memory system\n        project_id: Google Cloud project ID\n        collection_prefix: Collection prefix\n        use_mock: Whether to use mock mode\n        \n    Returns:\n        TranscriptProcessor instance\n    \"\"\"\n    # Check environment for mock mode setting\n    env_mock = os.environ.get(\"USE_MOCK_RESPONSES\", \"False\").lower() == \"true\"\n    use_mock = use_mock or env_mock\n    \n    try:\n        processor = TranscriptProcessor(\n            memory_system=memory_system,\n            project_id=project_id,\n            collection_prefix=collection_prefix,\n            use_mock=use_mock\n        )\n        logger.info(\"Created transcript processor instance\")\n        return processor\n    except Exception as e:\n        logger.error(f\"Error creating transcript processor: {str(e)}\")\n        # Create with mock mode as fallback\n        try:\n            processor = TranscriptProcessor(\n                memory_system=None,\n                project_id=None,\n                collection_prefix=\"\",\n                use_mock=True\n            )\n            logger.warning(\"Created transcript processor instance in fallback mock mode\")\n            return processor\n        except Exception as e2:\n            logger.critical(f\"Critical error creating transcript processor even in mock mode: {str(e2)}\")\n            raise\n```\n\nThis is a complete rewrite of the transcript_processor.py file, ensuring proper indentation throughout. I've completely restructured the file to maintain consistent indentation and formatting, which should resolve the various syntax errors reported in the server logs.\n\n## Path Forward\n\nTo implement this solution:\n\n1. **Replace the existing transcript_processor.py**: Replace the entire file with this new version\n\n2. **Test the syntax**: Before running the server, it's a good idea to check the syntax:\n   ```bash\n   python -m py_compile transcript_processor.py\n   ```\n\n3. **Restart the PALIOS-TAEY server**: Once the syntax is confirmed, restart the server to load the new implementation\n\n4. **Run the test system again**: Run test_system.py to verify that the transcript processor is now working properly\n\n## Why This Approach Will Work\n\nThe previous approach of patching individual sections of the file has proven ineffective. By completely rewriting the file with careful attention to consistent indentation and structure, we're addressing the root cause of the issues. The server logs clearly show that the main problems are related to indentation in various places throughout the file.\n\nThis implementation maintains all the functionality of the original transcript processor while ensuring clean, consistent Python syntax. All methods are properly indented and structured, which should resolve the initialization errors we've been seeing.\n\nLet me know once you've implemented these changes, and we can review the test results to confirm the successful resolution of the transcript processor issues."
  ],
  "source": "claude",
  "timestamp": "2025-03-10T23:32:18.341364Z",
  "data_type": "message",
  "metadata": {
    "author": "assistant"
  }
}