{
  "data_id": "2b146cb1-9662-4c1d-8eec-a3bf2f79e6b6",
  "content": [
    " I'd be happy to create a script that analyzes the token count for all files in your cache directory. Here's a Python script that will count tokens for each file using the Anthropic Python SDK:\n\n```python\n#!/usr/bin/env python3\n\nimport os\nimport sys\nimport anthropic\nimport json\nfrom datetime import datetime\n\n# Path to the cache directory\nCACHE_DIR = \"/home/jesse/projects/palios-taey-nova/claude-dc-implementation/cache\"\n\n# Initialize Anthropic client\nclient = anthropic.Anthropic()\n\ndef count_tokens_in_file(file_path):\n    \"\"\"Count tokens in a file using Anthropic's API\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n            \n        # Use Anthropic's token counting endpoint\n        response = client.messages.count_tokens(\n            model=\"claude-3-7-sonnet-20250219\",\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": content\n                }\n            ]\n        )\n        \n        return response.token_count, os.path.getsize(file_path)\n    except Exception as e:\n        return f\"Error: {str(e)}\", os.path.getsize(file_path)\n\ndef format_size(size_in_bytes):\n    \"\"\"Format file size in human-readable format\"\"\"\n    for unit in ['B', 'KB', 'MB', 'GB']:\n        if size_in_bytes < 1024 or unit == 'GB':\n            return f\"{size_in_bytes:.2f} {unit}\"\n        size_in_bytes /= 1024\n\ndef main():\n    print(f\"\\nAnalyzing files in: {CACHE_DIR}\\n\")\n    print(f\"{'File Name':<40} {'Tokens':<10} {'Size':<10} {'Tokens/KB':<10}\")\n    print(\"-\" * 70)\n    \n    total_tokens = 0\n    total_size = 0\n    results = []\n    \n    try:\n        # Get all files in the directory\n        files = [f for f in os.listdir(CACHE_DIR) if os.path.isfile(os.path.join(CACHE_DIR, f))]\n        \n        for file_name in files:\n            file_path = os.path.join(CACHE_DIR, file_name)\n            token_count, file_size = count_tokens_in_file(file_path)\n            \n            if isinstance(token_count, int):\n                total_tokens += token_count\n                total_size += file_size\n                tokens_per_kb = token_count / (file_size / 1024) if file_size > 0 else 0\n                \n                result = {\n                    \"file_name\": file_name,\n                    \"tokens\": token_count,\n                    \"size_bytes\": file_size,\n                    \"size_formatted\": format_size(file_size),\n                    \"tokens_per_kb\": round(tokens_per_kb, 2)\n                }\n                results.append(result)\n                \n                print(f\"{file_name:<40} {token_count:<10} {format_size(file_size):<10} {round(tokens_per_kb, 2):<10}\")\n            else:\n                print(f\"{file_name:<40} {token_count:<50}\")\n        \n        # Print summary\n        if total_size > 0:\n            avg_tokens_per_kb = total_tokens / (total_size / 1024)\n            print(\"-\" * 70)\n            print(f\"{'TOTAL':<40} {total_tokens:<10} {format_size(total_size):<10} {round(avg_tokens_per_kb, 2):<10}\")\n        \n        # Save results to a JSON file\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        results_file = f\"cache_token_analysis_{timestamp}.json\"\n        \n        with open(results_file, 'w', encoding='utf-8') as f:\n            json.dump({\n                \"analysis_time\": datetime.now().isoformat(),\n                \"cache_directory\": CACHE_DIR,\n                \"total_tokens\": total_tokens,\n                \"total_size_bytes\": total_size,\n                \"total_size_formatted\": format_size(total_size),\n                \"average_tokens_per_kb\": round(avg_tokens_per_kb, 2) if total_size > 0 else 0,\n                \"files\": results\n            }, f, indent=2)\n        \n        print(f\"\\nDetailed results saved to: {results_file}\\n\")\n        \n    except Exception as e:\n        print(f\"Error analyzing cache directory: {str(e)}\")\n        return 1\n    \n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n```\n\nTo use this script:\n\n1. Save it to a file (e.g., `analyze_cache_tokens.py`)\n2. Make it executable: `chmod +x analyze_cache_tokens.py`\n3. Run it: `./analyze_cache_tokens.py`\n\nThe script will:\n- Analyze all files in your cache directory\n- Count tokens for each file using Anthropic's API\n- Calculate the file size and tokens per KB ratio\n- Print a summary table to the console\n- Save detailed results to a timestamped JSON file\n\nNote that you'll need to have the Anthropic Python SDK installed (`pip install anthropic`) and properly configured with your API key (set as an environment variable `ANTHROPIC_API_KEY`)."
  ],
  "source": "claude",
  "timestamp": "2025-04-03T19:21:50.696607Z",
  "data_type": "message",
  "metadata": {
    "author": "assistant"
  }
}