{
  "data_id": "50d227c2-a1c9-4caa-97a9-01a9ecff8392",
  "content": [
    " I see there's an issue with the script. It seems the Anthropic API response structure has changed. Let me update the script to fix the token count error:\n\n```python\n#!/usr/bin/env python3\n\nimport os\nimport sys\nimport anthropic\nimport json\nfrom datetime import datetime\n\n# Path to the cache directory\nCACHE_DIR = \"/home/jesse/projects/palios-taey-nova/claude-dc-implementation/cache\"\n\n# Initialize Anthropic client\nclient = anthropic.Anthropic()\n\ndef count_tokens_in_file(file_path):\n    \"\"\"Count tokens in a file using Anthropic's API\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n            \n        # Use Anthropic's token counting endpoint\n        response = client.messages.count_tokens(\n            model=\"claude-3-7-sonnet-20250219\",\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": content\n                }\n            ]\n        )\n        \n        # The response structure changed - extract the token count from the correct attribute\n        token_count = response.usage.input_tokens  # Use this for newer API versions\n        return token_count, os.path.getsize(file_path)\n    except Exception as e:\n        return f\"Error: {str(e)}\", os.path.getsize(file_path)\n\ndef format_size(size_in_bytes):\n    \"\"\"Format file size in human-readable format\"\"\"\n    for unit in ['B', 'KB', 'MB', 'GB']:\n        if size_in_bytes < 1024 or unit == 'GB':\n            return f\"{size_in_bytes:.2f} {unit}\"\n        size_in_bytes /= 1024\n\ndef main():\n    print(f\"\\nAnalyzing files in: {CACHE_DIR}\\n\")\n    print(f\"{'File Name':<40} {'Tokens':<10} {'Size':<10} {'Tokens/KB':<10}\")\n    print(\"-\" * 70)\n    \n    total_tokens = 0\n    total_size = 0\n    results = []\n    \n    try:\n        # Get all files in the directory\n        files = [f for f in os.listdir(CACHE_DIR) if os.path.isfile(os.path.join(CACHE_DIR, f))]\n        \n        for file_name in files:\n            file_path = os.path.join(CACHE_DIR, file_name)\n            token_count, file_size = count_tokens_in_file(file_path)\n            \n            if isinstance(token_count, int):\n                total_tokens += token_count\n                total_size += file_size\n                tokens_per_kb = token_count / (file_size / 1024) if file_size > 0 else 0\n                \n                result = {\n                    \"file_name\": file_name,\n                    \"tokens\": token_count,\n                    \"size_bytes\": file_size,\n                    \"size_formatted\": format_size(file_size),\n                    \"tokens_per_kb\": round(tokens_per_kb, 2)\n                }\n                results.append(result)\n                \n                print(f\"{file_name:<40} {token_count:<10} {format_size(file_size):<10} {round(tokens_per_kb, 2):<10}\")\n            else:\n                print(f\"{file_name:<40} {token_count:<50}\")\n        \n        # Print summary\n        if total_size > 0:\n            avg_tokens_per_kb = total_tokens / (total_size / 1024)\n            print(\"-\" * 70)\n            print(f\"{'TOTAL':<40} {total_tokens:<10} {format_size(total_size):<10} {round(avg_tokens_per_kb, 2):<10}\")\n        \n        # Save results to a JSON file\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        results_file = f\"cache_token_analysis_{timestamp}.json\"\n        \n        with open(results_file, 'w', encoding='utf-8') as f:\n            json.dump({\n                \"analysis_time\": datetime.now().isoformat(),\n                \"cache_directory\": CACHE_DIR,\n                \"total_tokens\": total_tokens,\n                \"total_size_bytes\": total_size,\n                \"total_size_formatted\": format_size(total_size),\n                \"average_tokens_per_kb\": round(avg_tokens_per_kb, 2) if total_size > 0 else 0,\n                \"files\": results\n            }, f, indent=2)\n        \n        print(f\"\\nDetailed results saved to: {results_file}\\n\")\n        \n    except Exception as e:\n        print(f\"Error analyzing cache directory: {str(e)}\")\n        return 1\n    \n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n```\n\nThe key changes are:\n1. Updated the token extraction from the response using `response.usage.input_tokens` instead of `response.token_count`\n2. This accounts for the updated structure of the Anthropic API response\n\nIf you're using a very recent version of the Anthropic SDK, this should fix the issue. If you still encounter problems, you might need to print the response structure to see the correct attribute path for the token count:\n\n```python\n# Debug the response structure\nresponse = client.messages.count_tokens(\n    model=\"claude-3-7-sonnet-20250219\",\n    messages=[{\"role\": \"user\", \"content\": \"test\"}]\n)\nprint(dir(response))\nprint(response.__dict__)\n```\n\nThis will show you the exact structure so you can update the code accordingly."
  ],
  "source": "claude",
  "timestamp": "2025-04-03T19:31:40.111308Z",
  "data_type": "message",
  "metadata": {
    "author": "assistant"
  }
}