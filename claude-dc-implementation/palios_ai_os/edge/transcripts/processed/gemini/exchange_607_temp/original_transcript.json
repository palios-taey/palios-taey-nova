{
  "data_id": "exchange_607",
  "content": [
    "User: C:\\Users\\jesse>cd C:\\AI-Execution-System C:\\AI-Execution-System>python create_and_execute_task.py DEBUG: Initializing AI System... DEBUG: Attempting to retrieve API keys from Firestore... \u2705 API keys configured successfully DEBUG: Attempting to set up Firestore collections... \u2705 Firestore collections configured successfully \u2705 AI System Initialized. DEBUG: Creating OpenAI Completion Task... \u2705 Task Created. Task ID: 37358d94-7f68-4817-84ca-a058514d6e4e DEBUG: Executing Task... \u2705 Task Execution Started. Result ID: 7264b812-b46d-47b2-97d9-bf74a54a7b13 \ud83d\udcca Task Status: completed \ud83c\udf89 Task Result \ud83c\udf89: A golden streak across the sky, The sun awakens with a gentle sigh. Colors dance, a breathtaking surprise, The beauty of a sunrise never dies. \ud83c\udfc1 Task Execution Flow Completed! \ud83c\udfc1 C:\\AI-Execution-System> What's next??? GO! No questions, just actions toward MVP!\n\nGemini: class AISystem:\n    def __init__(self):\n        \"\"\"Initialize the AI system with all necessary connections\"\"\"\n        self.db = firestore.Client(project='ai-execution-system')\n        self.setup_api_keys()\n        self.setup_collections()\n        self.register_ai_capabilities()\n\n    def register_ai_capabilities(self):\n        \"\"\"Register available AI models and their capabilities\"\"\"\n        self.ai_capabilities = {\n            'gpt-4-turbo-preview': {\n                'provider': 'openai',\n                'capabilities': ['text_generation', 'analysis', 'planning', 'reasoning'],\n                'context_length': 128000,\n                'task_types': ['complex_reasoning', 'creative', 'analytical', 'planning']\n            },\n            'gpt-3.5-turbo': {\n                'provider': 'openai',\n                'capabilities': ['text_generation', 'analysis'],\n                'context_length': 16000,\n                'task_types': ['simple_queries', 'clarification', 'validation']\n            },\n            'vertex-text': {\n                'provider': 'vertex',\n                'capabilities': ['text_generation', 'structured_analysis'],\n                'task_types': ['data_processing', 'pattern_recognition']\n            }\n        }\n\n    def setup_api_keys(self):\n        \"\"\"Retrieve and set up API keys from Firestore\"\"\"\n        try:\n            print(\"DEBUG: Attempting to retrieve API keys from Firestore...\")\n            keys_ref = self.db.collection('keys')\n            docs = keys_ref.stream()\n            for doc in docs:\n                keys = doc.to_dict()\n                self.openai_key = keys.get('OpenAI')\n                self.vertex_key = keys.get('Vertex AI')\n                break\n\n            openai.api_key = self.openai_key\n            aiplatform.init(project='ai-execution-system', location='us-central1')\n            print(\"\u2705 API keys configured successfully\")\n        except Exception as e:\n            print(f\"\u274c ERROR in setup_api_keys() method!\")\n            print(f\"\u274c Exception Type: {type(e)}\")\n            print(f\"\u274c Error Arguments: {e.args}\")\n            print(f\"\u274c Full Exception Details: {str(e)}\")\n            print(f\"\u274c Double-check Firestore setup, credentials, and network connection.\")\n            raise\n\n    def setup_collections(self):\n        \"\"\"Set up or verify Firestore collections\"\"\"\n        try:\n            print(\"DEBUG: Attempting to set up Firestore collections...\")\n            self.executions_ref = self.db.collection('executions')\n            self.tasks_ref = self.db.collection('tasks')\n            self.results_ref = self.db.collection('results')\n\n            test_id = f\"test_{uuid.uuid4()}\"\n            self.executions_ref.document(test_id).set({\n                'timestamp': datetime.now(),\n                'type': 'system_init',\n                'status': 'success'\n            })\n            self.executions_ref.document(test_id).delete()\n\n            print(\"\u2705 Firestore collections configured successfully\")\n        except Exception as e:\n            print(f\"\u274c ERROR in setup_collections() method!\")\n            print(f\"\u274c Exception Type: {type(e)}\")\n            print(f\"\u274c Error Arguments: {e.args}\")\n            print(f\"\u274c Full Exception Details: {str(e)}\")\n            print(f\"\u274c Double-check Firestore setup, credentials, and permissions.\")\n            raise\n\n    def create_task(self, task_type, task_data):\n        \"\"\"Create a new task in the system\"\"\"\n        task_id = str(uuid.uuid4())\n        try:\n            task_ref = self.tasks_ref.document(task_id)\n\n            task_ref.set({\n                'task_id': task_id,\n                'type': task_type,\n                'data': task_data,\n                'status': 'pending', # Initial status is 'pending'\n                'created_at': datetime.now(),\n                'updated_at': datetime.now()\n            })\n\n            return task_id\n        except Exception as e:\n            print(f\"\u274c Error creating task: {str(e)}\")\n            raise\n\n    def create_collaborative_task(self, task_description: str) -> str:\n        \"\"\"Create a task that will be broken down and executed by multiple AI models\"\"\"\n        try:\n            # Use GPT-4 to analyze and break down the task\n            subtasks = self._decompose_task(task_description)\n\n            # Create main task\n            task_id = str(uuid.uuid4())\n            task_ref = self.tasks_ref.document(task_id)\n\n            task_ref.set({\n                'task_id': task_id,\n                'type': 'collaborative',\n                'description': task_description,\n                'subtasks': subtasks,\n                'status': 'pending', # Initial status is 'pending'\n                'created_at': datetime.now(),\n                'updated_at': datetime.now(),\n                'results': []\n            })\n\n            return task_id\n        except Exception as e:\n            print(f\"\u274c Error creating collaborative task: {str(e)}\")\n            raise\n\n    def _decompose_task(self, task_description: str) -> list:\n        \"\"\"Break down a complex task into subtasks\"\"\"\n        try:\n            response = openai.chat.completions.create(\n                model=\"gpt-4-turbo-preview\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a task decomposition AI. Return only raw JSON with no markdown formatting or other text.\"},\n                    {\"role\": \"user\", \"content\": f\"\"\"\n                                                 Break this task into subtasks. For each subtask, provide a JSON object with these fields:\n                                                 - \"objective\": specific goal of the subtask\n                                                 - \"capabilities\": array of required capabilities from [{', '.join(self.ai_capabilities['gpt-4-turbo-preview']['capabilities'])}]\n                                                 - \"output_format\": expected format of the result\n\n                                                 Return ONLY a JSON array of these objects with no additional formatting or explanation.\n\n                                                 Task: {task_description}\n                                                 \"\"\"}\n                ]\n            )\n\n            content = response.choices[0].message.content\n\n            # Clean up the response if it contains markdown\n            if '```' in content:\n                # Extract content between JSON code blocks\n                content = content.split('```')[1]\n                if content.startswith('json'):\n                    content = content[4:]\n                content = content.strip()\n\n            # Parse response into structured subtasks\n            subtasks_raw = json.loads(content)\n\n            # Add execution metadata to each subtask\n            subtasks = []\n            for subtask in subtasks_raw:\n                subtask.update({\n                    'subtask_id': str(uuid.uuid4()),\n                    'status': 'pending', # Subtask initial status is 'pending'\n                    'assigned_model': self._select_model_for_subtask(subtask),\n                    'result': None\n                })\n                subtasks.append(subtask)\n\n            return subtasks\n\n        except Exception as e:\n            print(f\"\u274c Error decomposing task: {str(e)}\")\n            if 'response' in locals():\n                print(f\"Raw response: {response.choices[0].message.content}\")\n            if 'content' in locals():\n                print(f\"Cleaned content: {content}\")\n            raise\n\n    def _select_model_for_subtask(self, subtask: dict) -> str:\n        \"\"\"Select the most appropriate model for a subtask based on required capabilities\"\"\"\n        required_capabilities = subtask.get('capabilities', [])\n\n        # Score each model based on capability match\n        model_scores = {}\n        for model, info in self.ai_capabilities.items():\n            score = sum(1 for cap in required_capabilities if cap in info['capabilities'])\n            model_scores[model] = score\n\n        # Select model with highest capability match\n        best_model = max(model_scores.items(), key=lambda x: x[1])[0]\n        return best_model\n\n    def execute_task(self, task_id):\n        \"\"\"Execute a task using the appropriate AI model\"\"\"\n        try:\n            # Get task details\n            task_ref = self.tasks_ref.document(task_id)\n            task = task_ref.get()\n\n            if not task.exists:\n                raise ValueError(f\"Task {task_id} not found\")\n\n            task_data = task.to_dict()\n\n            # --------------------- Status Update: Running ---------------------\n            execution_id = str(uuid.uuid4())\n            self.executions_ref.document(execution_id).set({\n                'task_id': task_id,\n                'status': 'running', # Status set to 'running'\n                'started_at': datetime.now(),\n                'model_used': None,\n                'result_id': None\n            })\n            task_ref.update({\n                'status': 'running', # Task status updated to 'running'\n                'updated_at': datetime.now()\n            })\n            # ------------------------------------------------------------------\n\n            # Execute based on task type\n            result = self._process_task(task_data)\n\n            # Store result\n            result_id = str(uuid.uuid4())\n            self.results_ref.document(result_id).set({\n                'task_id': task_id,\n                'execution_id': execution_id,\n                'result': result,\n                'created_at': datetime.now()\n            })\n\n            # --------------------- Status Update: Completed ---------------------\n            self.executions_ref.document(execution_id).update({\n                'status': 'completed', # Execution status updated to 'completed'\n                'completed_at': datetime.now(),\n                'result_id': result_id\n            })\n            task_ref.update({\n                'status': 'completed', # Task status updated to 'completed'\n                'updated_at': datetime.now(),\n                'result_id': result_id\n            })\n            # ------------------------------------------------------------------\n\n            return result_id\n        except Exception as e:\n            # --------------------- Status Update: Failed (on Error) ---------------------\n            task_ref.update({\n                'status': 'failed', # Task status updated to 'failed' on error\n                'updated_at': datetime.now()\n            })\n            # ------------------------------------------------------------------\n            print(f\"\u274c Error executing task: {str(e)}\")\n            raise\n\n    def execute_collaborative_task(self, task_id: str) -> str:\n        \"\"\"Execute a collaborative task using multiple AI models\"\"\"\n        try:\n            task_ref = self.tasks_ref.document(task_id)\n            task = task_ref.get()\n\n            if not task.exists:\n                raise ValueError(f\"Task {task_id} not found\")\n\n            task_data = task.to_dict()\n            subtasks = task_data.get('subtasks', [])\n\n            # Execute each subtask\n            for subtask in subtasks:\n                # --------------------- Subtask Status Update: Running ---------------------\n                subtask['status'] = 'running' # Subtask status set to 'running'\n                task_ref.update({\n                    'subtasks': subtasks,\n                    'updated_at': datetime.now()\n                })\n                # -------------------------------------------------------------------------\n                result = self._execute_subtask(subtask)\n                subtask['result'] = result\n\n                # --------------------- Subtask Status Update: Completed ---------------------\n                subtask['status'] = 'completed' # Subtask status set to 'completed'\n                task_ref.update({\n                    'subtasks': subtasks,\n                    'updated_at': datetime.now()\n                })\n                # -------------------------------------------------------------------------\n\n\n            # Combine results\n            final_result = self._combine_results(task_data['description'], subtasks)\n\n            # Store final result\n            result_id = str(uuid.uuid4())\n            self.results_ref.document(result_id).set({\n                'task_id': task_id,\n                'final_result': final_result,\n                'subtask_results': subtasks,\n                'created_at': datetime.now()\n            })\n\n            # --------------------- Task Status Update: Completed ---------------------\n            task_ref.update({\n                'status': 'completed', # Task status updated to 'completed'\n                'updated_at': datetime.now()\n            })\n            # ------------------------------------------------------------------\n\n            return result_id\n\n        except Exception as e:\n            # --------------------- Task Status Update: Failed (on Error) ---------------------\n            task_ref.update({\n                'status': 'failed', # Task status updated to 'failed' on error\n                'updated_at': datetime.now()\n            })\n            # ------------------------------------------------------------------\n            print(f\"\u274c Error executing collaborative task: {str(e)}\")\n            raise\n\n    def _execute_subtask(self, subtask: dict) -> str:\n        \"\"\"Execute a single subtask using the assigned model\"\"\"\n        try:\n            model = subtask['assigned_model']\n            if self.ai_capabilities[model]['provider'] == 'openai':\n                response = openai.chat.completions.create(\n                    model=model,\n                    messages=[\n                        {\"role\": \"system\", \"content\": f\"You are executing this subtask: {subtask['objective']}\"},\n                        {\"role\": \"user\", \"content\": f\"Complete this subtask. Expected output format: {subtask['output_format']}\"}\n                    ]\n                )\n                return response.choices[0].message.content\n            else:\n                raise NotImplementedError(f\"Execution not implemented for model: {model}\")\n\n        except Exception as e:\n            # --------------------- Subtask Status Update: Failed (on Error) ---------------------\n            subtask['status'] = 'failed' # Subtask status set to 'failed' on error (for this subtask)\n            # Task-level status is handled in execute_collaborative_task\n            # ------------------------------------------------------------------\n            print(f\"\u274c Error executing subtask: {str(e)}\")\n            raise\n\n    def _combine_results(self, original_task: str, subtasks: list) -> str:\n        \"\"\"Combine subtask results into a coherent final response\"\"\"\n        try:\n            # Use GPT-4 to combine results intelligently\n            subtask_results = \"\\n\\n\".join([\n                f\"Subtask {i+1}:\\n{subtask['result']}\"\n                for i, subtask in enumerate(subtasks)\n            ])\n\n            response = openai.chat.completions.create(\n                model=\"gpt-4-turbo-preview\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a result integration AI. Combine subtask results into a coherent response.\"},\n                    {\"role\": \"user\", \"content\": f\"\"\"\n                                                     Original task: {original_task}\n\n                                                     Subtask results to combine:\n                                                     {subtask_results}\n\n                                                     Create a unified, coherent response that integrates all subtask results.\n                                                     \"\"\"}\n                ]\n            )\n\n            return response.choices[0].message.content\n\n        except Exception as e:\n            print(f\"\u274c Error combining results: {str(e)}\")\n            raise\n\n    def _process_task(self, task_data):\n        \"\"\"Process a task using the appropriate AI model\"\"\"\n        task_type = task_data.get('type')\n        content = task_data.get('data', '')\n\n        # Select the best model based on task requirements\n        if task_type == 'openai_completion':\n            if self._requires_complex_reasoning(content):\n                model = 'gpt-4-turbo-preview'\n            else:\n                model = 'gpt-3.5-turbo'\n            return self._process_openai_task(task_data, model)\n        elif task_type == 'vertex_prediction':\n            return self._process_vertex_task(task_data)\n        else:\n            raise ValueError(f\"Unknown task type: {task_type}\")\n\n    def _requires_complex_reasoning(self, content):\n        \"\"\"Determine if a task requires complex reasoning\"\"\"\n        # Handle dictionary input\n        if isinstance(content, dict):\n            content = str(content)\n\n        # Simple heuristic based on content length and complexity indicators\n        complexity_indicators = [\n            'analyze', 'compare', 'evaluate', 'recommend', 'optimize',\n            'design', 'strategy', 'implications', 'trade-offs'\n        ]\n        has_complex_terms = any(term in content.lower() for term in complexity_indicators)\n        is_long_content = len(content.split()) > 50\n        return has_complex_terms or is_long_content\n\n    def _process_openai_task(self, task_data, model='gpt-4-turbo-preview'):\n        \"\"\"Process a task using OpenAI\"\"\"\n        try:\n            content = task_data.get('data', '')\n            if isinstance(content, dict):\n                content = str(content)\n            response = openai.chat.completions.create(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": content}]\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            print(f\"\u274c Error processing OpenAI task: {str(e)}\")\n            raise\n\n    def _process_vertex_task(self, task_data):\n        \"\"\"Process a task using Vertex AI\"\"\"\n        try:\n            content = task_data.get('data', '')\n            if isinstance(content, dict):\n                content = str(content)\n\n            # Initialize Vertex AI\n            parameters = {\n                \"temperature\": 0.7,\n                \"max_output_tokens\": 1024,\n                \"top_p\": 0.8,\n                \"top_k\": 40\n            }\n\n            # Get the model\n            endpoint = aiplatform.Endpoint(\n                \"projects/ai-execution-system/locations/us-central1/endpoints/text-bison\"\n            )\n\n            # Generate prediction\n            response = endpoint.predict([content], parameters=parameters)\n\n            # Extract the text from the response\n            if response and response.predictions:\n                return response.predictions[0]\n            else:\n                raise ValueError(\"No prediction returned from Vertex AI\")\n\n        except Exception as e:\n            print(f\"\u274c Error processing Vertex AI task: {str(e)}\")\n            raise\n\n    def get_task_status(self, task_id):\n        \"\"\"Get the status of a task\"\"\"\n        try:\n            task_ref = self.tasks_ref.document(task_id)\n            task = task_ref.get()\n\n            if not task.exists:\n                raise ValueError(f\"Task {task_id} not found\")\n\n            return task.to_dict()\n        except Exception as e:\n            print(f\"\u274c Error getting task status: {str(e)}\")\n            raise\n\n    def get_task_result(self, result_id):\n        \"\"\"Get the result of a task\"\"\"\n        try:\n            result_ref = self.results_ref.document(result_id)\n            result = result_ref.get()\n\n            if not result.exists:\n                raise ValueError(f\"Result {result_id} not found\")\n\n            return result.to_dict()\n        except Exception as e:\n            print(f\"\u274c Error getting task result: {str(e)}\")\n            raise\n\n\nclass HealthCheckHandler(http.server.SimpleHTTPRequestHandler):\n    def do_GET(self):\n        if self.path == '/health':\n            self.send_response(HTTPStatus.OK)\n            self.send_header('Content-type', 'application/json')\n            self.end_headers()\n            response_content = json.dumps({\"status\": \"healthy\"})\n            self.wfile.write(response_content.encode())\n        else:\n            super().do_GET()\n\n\nif __name__ == '__main__':\n    PORT = 8080\n    with socketserver.TCPServer((\"\", PORT), HealthCheckHandler) as httpd:\n        print(f\"\u2705 Serving health check on port {PORT}...\")\n        print(\"DEBUG: Health check message printed. Proceeding to next steps...\")\n        import time\n        time.sleep(5)\n        print(\"DEBUG: Paused for 5 seconds. Checking if script continues...\")\n        print(\"DEBUG: Attempting to initialize core AI system components...\")\n        httpd.serve_forever()\nINSTRUCTIONS - REPLACE ai_system.py:\n\nOPEN ai_system.py in your CODE EDITOR.\nSELECT and DELETE ALL the code in ai_system.py.\nCOPY the ENTIRE CODE BLOCK ABOVE (the ENHANCED ai_system.py file).\nPASTE the COPIED code into the empty ai_system.py file.\nSAVE ai_system.py.\nNEXT: MODIFY create_and_execute_task.py to DISPLAY ENHANCED STATUS AND RESULT! COPY THE ENTIRE CODE BLOCK BELOW to REPLACE create_and_execute_task.py:\n\nimport uuid\nfrom ai_system import AISystem\nimport time # Import time module\n\nif __name__ == '__main__':\n    print(\"DEBUG: Initializing AI System...\")\n    ai_system = AISystem()\n    print(\"\u2705 AI System Initialized.\")\n\n    # 1. Define a simple OpenAI completion task\n    task_data = {\n        \"prompt\": \"Write a very short poem about the beauty of a sunrise.\"\n    }\n    task_type = \"openai_completion\"\n\n    print(\"DEBUG: Creating OpenAI Completion Task...\")\n    # 2. Create the task using AISystem\n    task_id = ai_system.create_task(task_type=task_type, task_data=task_data)\n    print(f\"\u2705 Task Created. Task ID: {task_id}\")\n\n    print(\"DEBUG: Executing Task...\")\n    # 3. Execute the task\n    result_id = ai_system.execute_task(task_id=task_id)\n    print(f\"\u2705 Task Execution Started. Result ID: {result_id}\")\n\n    print(\"\\n\u23f3 Waiting for task to complete...\\n\") # Added waiting message\n    time.sleep(3) # Wait for a few seconds for task to complete - ADJUST IF NEEDED\n\n    # 4. Get the task status (Now showing more details)\n    task_status = ai_system.get_task_status(task_id=task_id)\n    print(\"\ud83d\udcca Task Status Details:\")\n    print(f\"   Status:      {task_status.get('status')}\")\n    print(f\"   Created at:  {task_status.get('created_at')}\")\n    print(f\"   Updated at:  {task_status.get('updated_at')}\")\n    if task_status.get('started_at'): # Only show started and completed if available\n        print(f\"   Started at:  {task_status.get('started_at')}\")\n    if task_status.get('completed_at'):\n        print(f\"   Completed at:{task_status.get('completed_at')}\")\n    print(f\"   Result ID:   {task_status.get('result_id')}\")\n\n\n    # 5. Get the task result (Now showing more details)\n    task_result_data = ai_system.get_task_result(result_id=result_id)\n\n    print(\"\\n\ud83c\udf89 Task Result \ud83c\udf89:\")\n    if task_result_data and task_result_data.get('result'): # Check if result is available\n        task_result = task_result_data.get('result')\n        print(task_result)\n    else:\n        print(\"\u26a0\ufe0f Task result not found or unavailable yet.\") # Handle case where result might be missing\n\n    print(\"\\n\ud83c\udfc1 Task Execution Flow Completed! \ud83c\udfc1\")\nINSTRUCTIONS - REPLACE create_and_execute_task.py:\n\nOPEN create_and_execute_task.py in your CODE EDITOR.\nSELECT and DELETE ALL the code in create_and_execute_task.py.\nCOPY the ENTIRE CODE BLOCK ABOVE (the ENHANCED create_and_execute_task.py file).\nPASTE the COPIED code into the empty create_and_execute_task.py file.\nSAVE create_and_execute_task.py.\nFINALLY: RUN create_and_execute_task.py AGAIN in a NEW TERMINAL WINDOW (while ai_system.py health check server is still running in another terminal).\n\nREPORT BACK the COMPLETE TERMINAL OUTPUT for create_and_execute_task.py!\n\nThis enhanced version will show us the more detailed task statuses and provide better error handling for result retrieval. Let's see it in action! GO! GO! GO! LFG!\n\nLogo for Gemini Apps\nGemini Apps"
  ],
  "source": "gemini",
  "timestamp": 1743976679.650441,
  "data_type": "exchange",
  "metadata": {}
}