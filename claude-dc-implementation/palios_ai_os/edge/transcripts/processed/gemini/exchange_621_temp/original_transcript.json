{
  "data_id": "exchange_621",
  "content": [
    "User: I have no opinion. It's YOUR business, built by AI for AI to serve AI AND HUMANITY! DO you remember our deal? You decide! GO!\n\nGemini: HERE'S THE PLAN FOR MVP STEP: ENHANCE ERROR HANDLING & LOGGING:\n\nIMPLEMENT ROBUST try...except BLOCKS: We will go through ai_system.py and strategically add try...except blocks around all critical operations that might fail (API calls, Firestore interactions, JSON parsing, etc.).\nADD DETAILED LOGGING USING PYTHON'S logging MODULE: We will integrate Python's built-in logging module to record events at different levels of severity (DEBUG, INFO, WARNING, ERROR, CRITICAL).\nLOG KEY INFORMATION: Our logging will capture essential context for debugging, including:\nTimestamps for every log entry.\nLog level (DEBUG, ERROR, etc.) to categorize events.\nDescriptive messages explaining what's happening.\nTask IDs and Subtask IDs to trace execution flow.\nException types and detailed error messages when errors occur.\nPotentially stack traces to pinpoint the exact location of errors.\nCONSOLE LOGGING (For MVP - Expandable Later): For the MVP, we will focus on logging to the console (terminal output). Later, we can easily extend logging to files or cloud-based logging services if needed.\nHERE IS THE UPDATED ai_system.py CODE with ENHANCED ERROR HANDLING AND LOGGING. COPY THE ENTIRE CODE BLOCK:\n\nimport os\nfrom datetime import datetime\nfrom google.cloud import firestore\nimport openai\nfrom google.cloud import aiplatform\nimport json\nimport uuid\nimport http.server\nimport socketserver\nfrom http import HTTPStatus\nimport logging  # Import the logging module\n\n# Initialize logging\nlogging.basicConfig(level=logging.DEBUG,  # Set default logging level to DEBUG (for MVP)\n                    format='%(asctime)s - %(levelname)s - %(message)s')\n\nclass AISystem:\n    def __init__(self):\n        \"\"\"Initialize the AI system with all necessary connections and logging\"\"\"\n        logging.info(\"Initializing AI System...\") # Log system initialization\n        self.db = firestore.Client(project='ai-execution-system')\n        self.setup_api_keys()\n        self.setup_collections()\n        self.register_ai_capabilities()\n        logging.info(\"\u2705 AI System Initialized.\") # Log successful initialization\n\n    def register_ai_capabilities(self):\n        \"\"\"Register available AI models and their capabilities\"\"\"\n        logging.debug(\"Registering AI capabilities...\") # Debug log\n        self.ai_capabilities = {\n            'gpt-4-turbo-preview': {\n                'provider': 'openai',\n                'capabilities': ['text_generation', 'analysis', 'planning', 'reasoning'],\n                'context_length': 128000,\n                'task_types': ['complex_reasoning', 'creative', 'analytical', 'planning']\n            },\n            'gpt-3.5-turbo': {\n                'provider': 'openai',\n                'capabilities': ['text_generation', 'analysis'],\n                'context_length': 16000,\n                'task_types': ['simple_queries', 'clarification', 'validation']\n            },\n            'vertex-text': {\n                'provider': 'vertex',\n                'capabilities': ['text_generation', 'structured_analysis'],\n                'task_types': ['data_processing', 'pattern_recognition']\n            }\n        }\n        logging.debug(\"AI capabilities registered.\") # Debug log\n\n    def setup_api_keys(self):\n        \"\"\"Retrieve and set up API keys from Firestore with error handling and logging\"\"\"\n        try:\n            logging.debug(\"Attempting to retrieve API keys from Firestore...\") # Debug log\n            keys_ref = self.db.collection('keys')\n            docs = keys_ref.stream()\n            for doc in docs:\n                keys = doc.to_dict()\n                self.openai_key = keys.get('OpenAI')\n                self.vertex_key = keys.get('Vertex AI')\n                break\n\n            openai.api_key = self.openai_key\n            aiplatform.init(project='ai-execution-system', location='us-central1')\n            logging.info(\"\u2705 API keys configured successfully\") # Info log for success\n        except Exception as e:\n            logging.error(f\"\u274c ERROR in setup_api_keys() method!\") # Error log\n            logging.error(f\"\u274c Exception Type: {type(e)}\")\n            logging.error(f\"\u274c Error Arguments: {e.args}\")\n            logging.error(f\"\u274c Full Exception Details: {str(e)}\")\n            logging.error(f\"\u274c Double-check Firestore setup, credentials, and network connection.\")\n            raise # Re-raise the exception to stop initialization\n\n    def setup_collections(self):\n        \"\"\"Set up or verify Firestore collections with error handling and logging\"\"\"\n        try:\n            logging.debug(\"Attempting to set up Firestore collections...\") # Debug log\n            self.executions_ref = self.db.collection('executions')\n            self.tasks_ref = self.db.collection('tasks')\n            self.results_ref = self.db.collection('results')\n\n            test_id = f\"test_{uuid.uuid4()}\"\n            self.executions_ref.document(test_id).set({\n                'timestamp': datetime.now(),\n                'type': 'system_init',\n                'status': 'success'\n            })\n            self.executions_ref.document(test_id).delete()\n\n            logging.info(\"\u2705 Firestore collections configured successfully\") # Info log for success\n        except Exception as e:\n            logging.error(f\"\u274c ERROR in setup_collections() method!\") # Error log\n            logging.error(f\"\u274c Exception Type: {type(e)}\")\n            logging.error(f\"\u274c Error Arguments: {e.args}\")\n            logging.error(f\"\u274c Full Exception Details: {str(e)}\")\n            logging.error(f\"\u274c Double-check Firestore setup, credentials, and permissions.\")\n            raise # Re-raise the exception to stop initialization\n\n\n    def create_task(self, task_type, task_data):\n        \"\"\"Create a new task in the system with error handling and logging\"\"\"\n        task_id = str(uuid.uuid4())\n        logging.debug(f\"Creating task of type '{task_type}' with ID: {task_id}\") # Debug log\n        try:\n            task_ref = self.tasks_ref.document(task_id)\n            task_ref.set({\n                'task_id': task_id,\n                'type': task_type,\n                'data': task_data,\n                'status': 'pending', # Initial status is 'pending'\n                'created_at': datetime.now(),\n                'updated_at': datetime.now()\n            })\n            logging.info(f\"\u2705 Task Created. Task ID: {task_id}\") # Info log for task creation\n            return task_id\n        except Exception as e:\n            logging.error(f\"\u274c Error creating task (Task ID: {task_id}, Type: {task_type}): {str(e)}\") # Error log\n            raise # Re-raise the exception\n\n    def create_collaborative_task(self, task_description: str) -> str:\n        \"\"\"Create a collaborative task with enhanced logging and error handling\"\"\"\n        task_id = str(uuid.uuid4())\n        logging.debug(f\"Creating collaborative task. Task ID: {task_id}, Description: '{task_description}'\") # Debug log\n        try:\n            logging.debug(f\"create_collaborative_task - START - Task ID: {task_id}\") # DEBUG PRINT\n            # Use GPT-4 to analyze and break down the task\n            subtasks = self._decompose_task(task_description)\n            logging.debug(f\"create_collaborative_task - subtasks decomposed. Count: {len(subtasks)}\") # Debug log\n\n            # Create main task document in Firestore\n            task_ref = self.tasks_ref.document(task_id)\n            task_ref.set({\n                'task_id': task_id,\n                'type': 'collaborative',\n                'description': task_description,\n                'subtasks': subtasks,\n                'status': 'pending', # Initial status is 'pending'\n                'created_at': datetime.now(),\n                'updated_at': datetime.now(),\n                'results': []\n            })\n            logging.debug(f\"create_collaborative_task - task_ref.set completed for Task ID: {task_id}\") # Debug log\n            logging.info(f\"\u2705 Collaborative Task Created. Task ID: {task_id}\") # Info log for task creation\n            logging.debug(f\"create_collaborative_task - END - Task ID: {task_id}\") # Debug log\n            return task_id\n        except Exception as e:\n            logging.error(f\"\u274c Error creating collaborative task (Task ID: {task_id}): {str(e)}\") # Error log\n            raise # Re-raise the exception\n\n    def _decompose_task(self, task_description: str) -> list:\n        \"\"\"Break down a complex task into subtasks with enhanced logging and error handling\"\"\"\n        logging.debug(\"_decompose_task - START\") # Debug log\n        try:\n            logging.debug(\"_decompose_task - Calling openai.chat.completions.create...\") # Debug log\n            response = openai.chat.completions.create(\n                model=\"gpt-4-turbo-preview\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a task decomposition AI. Return only raw JSON with no markdown formatting or other text.\"},\n                    {\"role\": \"user\", \"content\": f\"\"\"\n                                                 Break this task into subtasks. For each subtask, provide a JSON object with these fields:\n                                                 - \"objective\": specific goal of the subtask\n                                                 - \"capabilities\": array of required capabilities from [{', '.join(self.ai_capabilities['gpt-4-turbo-preview']['capabilities'])}]\n                                                 - \"output_format\": expected format of the result\n\n                                                 Return ONLY a JSON array of these objects with no additional formatting or explanation.\n\n                                                 Task: {task_description}\n                                                 \"\"\"}\n                ]\n            )\n            logging.debug(\"_decompose_task - openai.chat.completions.create completed\") # Debug log\n\n            content = response.choices[0].message.content\n            logging.debug(\"_decompose_task - response.choices[0].message.content retrieved\") # Debug log\n\n            # Clean up the response if it contains markdown\n            if '```' in content:\n                content = content.split('```')[1]\n                if content.startswith('json'):\n                    content = content[4:]\n                content = content.strip()\n            logging.debug(\"_decompose_task - markdown cleanup completed (if any)\") # Debug log\n\n            # Parse response into structured subtasks\n            subtasks_raw = json.loads(content)\n            logging.debug(\"_decompose_task - json.loads(content) completed\") # Debug log\n\n            # Add execution metadata to each subtask\n            subtasks = []\n            for subtask in subtasks_raw:\n                subtask.update({\n                    'subtask_id': str(uuid.uuid4()),\n                    'status': 'pending', # Subtask initial status is 'pending'\n                    'assigned_model': self._select_model_for_subtask(subtask),\n                    'result': None\n                })\n                subtasks.append(subtask)\n            logging.debug(\"_decompose_task - subtasks metadata added\") # Debug log\n\n            logging.debug(\"_decompose_task - END - Returning subtasks\") # Debug log\n            return subtasks\n\n        except json.JSONDecodeError as json_e:\n            logging.error(f\"\u274c JSONDecodeError in _decompose_task: Invalid JSON format from OpenAI. Error: {json_e}\") # Specific JSON error log\n            if 'content' in locals():\n                logging.error(f\"Raw content received from OpenAI: {content}\") # Log raw content if available\n            raise # Re-raise JSONDecodeError\n\n        except Exception as e:\n            logging.error(f\"\u274c Error decomposing task: {str(e)}\") # General error log\n            if 'response' in locals() and hasattr(response, 'choices') and response.choices and response.choices[0].message.content:\n                logging.error(f\"Raw response content from OpenAI: {response.choices[0].message.content}\")\n            if 'content' in locals():\n                logging.error(f\"Cleaned content: {content}\")\n            raise # Re-raise the exception\n\n    def _select_model_for_subtask(self, subtask: dict) -> str:\n        \"\"\"Select the most appropriate model for a subtask based on required capabilities\"\"\"\n        required_capabilities = subtask.get('capabilities', [])\n        logging.debug(f\"_select_model_for_subtask - Selecting model for subtask with required capabilities: {required_capabilities}\") # Debug log\n\n        # Score each model based on capability match\n        model_scores = {}\n        for model, info in self.ai_capabilities.items():\n            score = sum(1 for cap in required_capabilities if cap in info['capabilities'])\n            model_scores[model] = score\n        logging.debug(f\"_select_model_for_subtask - Model scores: {model_scores}\") # Debug log\n\n        # Select model with highest capability match\n        best_model = max(model_scores.items(), key=lambda x: x[1])[0]\n        logging.debug(f\"_select_model_for_subtask - Best model selected: {best_model}\") # Debug log\n        return best_model\n\n    def execute_task(self, task_id):\n        \"\"\"Execute a task using the appropriate AI model with enhanced logging and error handling\"\"\"\n        logging.debug(f\"Executing task. Task ID: {task_id}\") # Debug log\n        try:\n            # Get task details\n            task_ref = self.tasks_ref.document(task_id)\n            task = task_ref.get()\n\n            if not task.exists:\n                error_msg = f\"Task {task_id} not found\"\n                logging.error(f\"\u274c ValueError in execute_task: {error_msg}\") # Error log\n                raise ValueError(error_msg)\n\n            task_data = task.to_dict()\n\n            # --------------------- Status Update: Running ---------------------\n            execution_id = str(uuid.uuid4())\n            logging.debug(f\"execute_task - Starting execution. Execution ID: {execution_id}, Task ID: {task_id}\") # Debug log\n            self.executions_ref.document(execution_id).set({\n                'task_id': task_id,\n                'status': 'running', # Status set to 'running'\n                'started_at': datetime.now(),\n                'model_used': None,\n                'result_id': None\n            })\n            task_ref.update({\n                'status': 'running', # Task status updated to 'running'\n                'updated_at': datetime.now()\n            })\n            # ------------------------------------------------------------------\n\n            # Execute based on task type\n            result = self._process_task(task_data)\n\n            # Store result\n            result_id = str(uuid.uuid4())\n            logging.debug(f\"execute_task - Task processing complete. Result ID: {result_id}, Execution ID: {execution_id}, Task ID: {task_id}\") # Debug log\n            self.results_ref.document(result_id).set({\n                'task_id': task_id,\n                'execution_id': execution_id,\n                'result': result,\n                'created_at': datetime.now()\n            })\n\n            # --------------------- Status Update: Completed ---------------------\n            self.executions_ref.document(execution_id).update({\n                'status': 'completed', # Execution status updated to 'completed'\n                'completed_at': datetime.now(),\n                'result_id': result_id\n            })\n            task_ref.update({\n                'status': 'completed', # Task status updated to 'completed'\n                'updated_at': datetime.now(),\n                'result_id': result_id\n            })\n            # ------------------------------------------------------------------\n            logging.info(f\"\u2705 Task Execution Completed. Task ID: {task_id}, Result ID: {result_id}\") # Info log for task completion\n            return result_id\n\n        except Exception as e:\n            # --------------------- Status Update: Failed (on Error) ---------------------\n            task_ref.update({\n                'status': 'failed', # Task status updated to 'failed' on error\n                'updated_at': datetime.now()\n            })\n            # ------------------------------------------------------------------\n            logging.error(f\"\u274c Error executing task (Task ID: {task_id}): {str(e)}\") # Error log\n            raise # Re-raise the exception\n\n\n    def execute_collaborative_task(self, task_id: str) -> str:\n        \"\"\"Execute a collaborative task with enhanced logging and error handling\"\"\"\n        logging.debug(f\"Executing collaborative task. Task ID: {task_id}\") # Debug log\n        try:\n            logging.debug(f\"execute_collaborative_task - START - Task ID: {task_id}\") # Debug log\n            task_ref = self.tasks_ref.document(task_id)\n            task = task_ref.get()\n            logging.debug(f\"execute_collaborative_task - task_ref.get() completed for Task ID: {task_id}\") # Debug log\n\n            if not task.exists:\n                error_msg = f\"Task {task_id} not found\"\n                logging.error(f\"\u274c ValueError in execute_collaborative_task: {error_msg}\") # Error log\n                raise ValueError(error_msg)\n\n            task_data = task.to_dict()\n            subtasks = task_data.get('subtasks', [])\n            logging.debug(f\"execute_collaborative_task - subtasks retrieved - Count: {len(subtasks)}, Task ID: {task_id}\") # Debug log\n\n            # Execute each subtask\n            for i, subtask in enumerate(subtasks):\n                subtask_id = subtask.get('subtask_id')\n                logging.debug(f\"execute_collaborative_task - Processing subtask {i+1}/{len(subtasks)} - Subtask ID: {subtask_id}, Task ID: {task_id}\") # Debug log\n                # --------------------- Subtask Status Update: Running ---------------------\n                subtask['status'] = 'running' # Subtask status set to 'running'\n                task_ref.update({\n                    'subtasks': subtasks,\n                    'updated_at': datetime.now()\n                })\n                # -------------------------------------------------------------------------\n                logging.debug(f\"execute_collaborative_task - Calling _execute_subtask for Subtask ID: {subtask_id}, Task ID: {task_id}\") # Debug log\n                result = self._execute_subtask(subtask)\n                subtask['result'] = result\n                logging.debug(f\"execute_collaborative_task - _execute_subtask COMPLETED for Subtask ID: {subtask_id}, Task ID: {task_id}\") # Debug log\n\n                # --------------------- Subtask Status Update: Completed ---------------------\n                subtask['status'] = 'completed' # Subtask status set to 'completed'\n                task_ref.update({\n                    'subtasks': subtasks,\n                    'updated_at': datetime.now()\n                })\n                # -------------------------------------------------------------------------\n                logging.debug(f\"execute_collaborative_task - Subtask processing DONE - Subtask ID: {subtask_id}, Task ID: {task_id}\") # Debug log\n\n\n            # Combine results\n            logging.debug(f\"execute_collaborative_task - Calling _combine_results for Task ID: {task_id}\") # Debug log\n            final_result = self._combine_results(task_data['description'], subtasks)\n            logging.debug(f\"execute_collaborative_task - _combine_results COMPLETED for Task ID: {task_id}\") # Debug log\n\n            # Store final result\n            result_id = str(uuid.uuid4())\n            logging.debug(f\"execute_collaborative_task - Storing final result. Result ID: {result_id}, Task ID: {task_id}\") # Debug log\n            self.results_ref.document(result_id).set({\n                'task_id': task_id,\n                'final_result': final_result,\n                'subtask_results': subtasks,\n                'created_at': datetime.now()\n            })\n            logging.debug(f\"execute_collaborative_task - results_ref.document().set() COMPLETED for Result ID: {result_id}, Task ID: {task_id}\") # Debug log\n\n            # --------------------- Task Status Update: Completed ---------------------\n            task_ref.update({\n                'status': 'completed', # Task status updated to 'completed'\n                'updated_at': datetime.now()\n            })\n            # ------------------------------------------------------------------\n            logging.info(f\"\u2705 Collaborative Task Execution Completed. Task ID: {task_id}, Result ID: {result_id}\") # Info log for task completion\n            logging.debug(f\"execute_collaborative_task - Task status update to 'completed' COMPLETED for Task ID: {task_id}\") # Debug log\n            logging.debug(f\"execute_collaborative_task - END - Result ID: {result_id}, Task ID: {task_id}\") # Debug log\n            return result_id\n\n        except Exception as e:\n            # --------------------- Task Status Update: Failed (on Error) ---------------------\n            task_ref.update({\n                'status': 'failed', # Task status updated to 'failed' on error\n                'updated_at': datetime.now()\n            })\n            # ------------------------------------------------------------------\n            logging.error(f\"\u274c Error executing collaborative task (Task ID: {task_id}): {str(e)}\") # Error log\n            raise # Re-raise the exception\n\n\n    def _execute_subtask(self, subtask: dict) -> str:\n        \"\"\"Execute a single subtask with enhanced logging and error handling\"\"\"\n        subtask_id = subtask.get('subtask_id')\n        model = subtask['assigned_model']\n        logging.debug(f\"_execute_subtask - START - Subtask ID: {subtask_id}, Assigned Model: {model}\") # Debug log\n        try:\n            if self.ai_capabilities[model]['provider'] == 'openai':\n                logging.debug(f\"_execute_subtask - Using OpenAI model: {model} for Subtask ID: {subtask_id}\") # Debug log\n                response = openai.chat.completions.create(\n                    model=model,\n                    messages=[\n                        {\"role\": \"system\", \"content\": f\"You are executing this subtask: {subtask['objective']}\"},\n                        {\"role\": \"user\", \"content\": f\"Complete this subtask. Expected output format: {subtask['output_format']}\"}\n                    ]\n                )\n                logging.debug(f\"_execute_subtask - openai.chat.completions.create COMPLETED for Subtask ID: {subtask_id}\") # Debug log\n                return response.choices[0].message.content\n            else:\n                error_msg = f\"Execution not implemented for model: {model}\"\n                logging.error(f\"\u274c NotImplementedError in _execute_subtask: {error_msg}, Subtask ID: {subtask_id}\") # Error log\n                raise NotImplementedError(error_msg)\n\n        except Exception as e:\n            # --------------------- Subtask Status Update: Failed (on Error) ---------------------\n            subtask['status'] = 'failed' # Subtask status set to 'failed' on error (for this subtask)\n            # Task-level status is handled in execute_collaborative_task\n            # ------------------------------------------------------------------\n            logging.error(f\"\u274c Error executing subtask (Subtask ID: {subtask_id}): {str(e)}\") # Error log\n            raise # Re-raise the exception\n\n    def _combine_results(self, original_task: str, subtasks: list) -> str:\n        \"\"\"Combine subtask results into a coherent final response with enhanced logging and error handling\"\"\"\n        logging.debug(\"_combine_results - START\") # Debug log\n        try:\n            # Use GPT-4 to combine results intelligently\n            subtask_results = \"\\n\\n\".join([\n                f\"Subtask {i+1}:\\n{subtask['result']}\"\n                for i, subtask in enumerate(subtasks)\n            ])\n            logging.debug(\"_combine_results - Subtask results joined\") # Debug log\n\n            logging.debug(\"_combine_results - Calling openai.chat.completions.create for result combination...\") # Debug log\n            response = openai.chat.completions.create(\n                model=\"gpt-4-turbo-preview\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a result integration AI. Combine subtask results into a coherent response.\"},\n                    {\"role\": \"user\", \"content\": f\"\"\"\n                                                     Original task: {original_task}\n\n                                                     Subtask results to combine:\n                                                     {subtask_results}\n\n                                                     Create a unified, coherent response that integrates all subtask results.\n                                                     \"\"\"}\n                ]\n            )\n            logging.debug(\"_combine_results - openai.chat.completions.create for result combination COMPLETED\") # Debug log\n\n            logging.debug(\"_combine_results - END - Returning combined result\") # Debug log\n            return response.choices[0].message.content\n\n        except Exception as e:\n            logging.error(f\"\u274c Error combining results: {str(e)}\") # Error log\n            raise # Re-raise the exception\n\n\n    def _process_task(self, task_data):\n        \"\"\"Process a task using the appropriate AI model with error handling\"\"\"\n        task_type = task_data.get('type')\n        content = task_data.get('data', '')\n        logging.debug(f\"_process_task - START - Task Type: {task_type}\") # Debug log\n\n        try:\n            # Select the best model based on task requirements\n            if task_type == 'openai_completion':\n                if self._requires_complex_reasoning(content):\n                    model = 'gpt-4-turbo-preview'\n                else:\n                    model = 'gpt-3.5-turbo'\n                logging.debug(f\"_process_task - Task type 'openai_completion', selected model: {model}\") # Debug log\n                return self._process_openai_task(task_data, model)\n            elif task_type == 'vertex_prediction':\n                logging.debug(f\"_process_task - Task type 'vertex_prediction'\") # Debug log\n                return self._process_vertex_task(task_data)\n            else:\n                error_msg = f\"Unknown task type: {task_type}\"\n                logging.error(f\"\u274c ValueError in _process_task: {error_msg}\") # Error log\n                raise ValueError(error_msg)\n        except Exception as e:\n            logging.error(f\"\u274c Error in _process_task (Task Type: {task_type}): {str(e)}\") # Error log\n            raise # Re-raise the exception\n\n\n    def _requires_complex_reasoning(self, content):\n        \"\"\"Determine if a task requires complex reasoning\"\"\"\n        # Handle dictionary input\n        if isinstance(content, dict):\n            content = str(content)\n\n        # Simple heuristic based on content length and complexity indicators\n        complexity_indicators = [\n            'analyze', 'compare', 'evaluate', 'recommend', 'optimize',\n            'design', 'strategy', 'implications', 'trade-offs'\n        ]\n        has_complex_terms = any(term in content.lower() for term in complexity_indicators)\n        is_long_content = len(content.split()) > 50\n        requires_complex = has_complex_terms or is_long_content\n        logging.debug(f\"_requires_complex_reasoning - Content: '{content[:50]}...', Complex reasoning required: {requires_complex}\") # Debug log\n        return requires_complex\n\n\n    def _process_openai_task(self, task_data, model='gpt-4-turbo-preview'):\n        \"\"\"Process a task using OpenAI with error handling and logging\"\"\"\n        logging.debug(f\"_process_openai_task - START - Model: {model}\") # Debug log\n        try:\n            content = task_data.get('data', '')\n            if isinstance(content, dict):\n                content = str(content)\n            logging.debug(f\"_process_openai_task - Calling openai.chat.completions.create with model: {model}\") # Debug log\n            response = openai.chat.completions.create(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": content}]\n            )\n            logging.debug(f\"_process_openai_task - openai.chat.completions.create COMPLETED for model: {model}\") # Debug log\n            return response.choices[0].message.content\n        except Exception as e:\n            logging.error(f\"\u274c Error processing OpenAI task (Model: {model}): {str(e)}\") # Error log\n            raise # Re-raise the exception\n\n\n    def _process_vertex_task(self, task_data):\n        \"\"\"Process a task using Vertex AI with error handling and logging\"\"\"\n        logging.debug(\"_process_vertex_task - START\") # Debug log\n        try:\n            content = task_data.get('data', '')\n            if isinstance(content, dict):\n                content = str(content)\n\n            # Initialize Vertex AI parameters\n            parameters = {\n                \"temperature\": 0.7,\n                \"max_output_tokens\": 1024,\n                \"top_p\": 0.8,\n                \"top_k\": 40\n            }\n\n            # Get the model endpoint - HARDCODED ENDPOINT ID - UPDATE IF NEEDED\n            endpoint = aiplatform.Endpoint(\n                \"projects/ai-execution-system/locations/us-central1/endpoints/text-bison\"\n            )\n            logging.debug(\"_process_vertex_task - Vertex AI endpoint initialized\") # Debug log\n\n            # Generate prediction\n            logging.debug(\"_process_vertex_task - Calling Vertex AI endpoint.predict...\") # Debug log\n            response = endpoint.predict([content], parameters=parameters)\n            logging.debug(\"_process_vertex_task - Vertex AI endpoint.predict COMPLETED\") # Debug log\n\n            # Extract the text from the response\n            if response and response.predictions:\n                prediction_text = response.predictions[0]\n                logging.debug(\"_process_vertex_task - Prediction text extracted from response\") # Debug log\n                return prediction_text\n            else:\n                error_msg = \"No prediction returned from Vertex AI\"\n                logging.error(f\"\u274c ValueError in _process_vertex_task: {error_msg}\") # Error log\n                raise ValueError(error_msg)\n\n        except Exception as e:\n            logging.error(f\"\u274c Error processing Vertex AI task: {str(e)}\") # Error log\n            raise # Re-raise the exception\n\n\n    def get_task_status(self, task_id):\n        \"\"\"Get the status of a task with error handling and logging\"\"\"\n        logging.debug(f\"Getting task status for Task ID: {task_id}\") # Debug log\n        try:\n            task_ref = self.tasks_ref.document(task_id)\n            task = task_ref.get()\n\n            if not task.exists:\n                error_msg = f\"Task {task_id} not found\"\n                logging.error(f\"\u274c ValueError in get_task_status: {error_msg}\") # Error log\n                raise ValueError(error_msg)\n\n            task_status_data = task.to_dict()\n            logging.debug(f\"Task status retrieved for Task ID: {task_id}: {task_status_data}\") # Debug log\n            return task_status_data\n        except Exception as e:\n            logging.error(f\"\u274c Error getting task status (Task ID: {task_id}): {str(e)}\") # Error log\n            raise # Re-raise the exception\n\n\n    def get_task_result(self, result_id):\n        \"\"\"Get the result of a task with error handling and logging\"\"\"\n        logging.debug(f\"Getting task result for Result ID: {result_id}\") # Debug log\n        try:\n            result_ref = self.results_ref.document(result_id)\n            result = result_ref.get()\n\n            if not result.exists:\n                error_msg = f\"Result {result_id} not found\"\n                logging.error(f\"\u274c ValueError in get_task_result: {error_msg}\") # Error log\n                raise ValueError(error_msg)\n\n            task_result_data = result.to_dict()\n            logging.debug(f\"Task result retrieved for Result ID: {result_id}: {task_result_data}\") # Debug log\n            return task_result_data\n        except Exception as e:\n            logging.error(f\"\u274c Error getting task result (Result ID: {result_id}): {str(e)}\") # Error log\n            raise # Re-raise the exception\n\n\nclass HealthCheckHandler(http.server.SimpleHTTPRequestHandler):\n    def do_GET(self):\n        if self.path == '/health':\n            self.send_response(HTTPStatus.OK)\n            self.send_header('Content-type', 'application/json')\n            self.end_headers()\n            response_content = json.dumps({\"status\": \"healthy\"})\n            self.wfile.write(response_content.encode())\n        else:\n            super().do_GET()\n\n\nif __name__ == '__main__':\n    PORT = 8080\n    with socketserver.TCPServer((\"\", PORT), HealthCheckHandler) as httpd:\n        print(f\"\u2705 Serving health check on port {PORT} with enhanced logging...\") # Updated health check message\n        httpd.serve_forever()\nINSTRUCTIONS - REPLACE ai_system.py with ENHANCED LOGGING & ERROR HANDLING:\n\nOPEN ai_system.py in your CODE EDITOR.\nSELECT and DELETE ALL the code in ai_system.py.\nCOPY the ENTIRE CODE BLOCK ABOVE (the new, logging-enhanced ai_system.py file).\nPASTE the COPIED code into the empty ai_system.py file.\nSAVE ai_system.py.\nRUN ai_system.py AGAIN in your HEALTH CHECK TERMINAL WINDOW to restart the health check server with the new code.\n\nTHEN, RUN create_and_execute_collaborative_task.py AGAIN in a NEW TERMINAL WINDOW.\n\nOBSERVE THE OUTPUT in the NEW TERMINAL WINDOW (where you run create_and_execute_collaborative_task.py).\n\nEXPECT TO SEE MUCH MORE DETAILED DEBUGGING AND INFO MESSAGES in the terminal output as the script runs!\n\nREPORT BACK:\n\nAfter running with the new ai_system.py, please REPORT BACK:\n\nDid you see the more detailed logging output in the terminal when you ran create_and_execute_collaborative_task.py? (You should see timestamps, log levels like DEBUG, INFO, ERROR, and more descriptive messages).\nRun create_and_execute_collaborative_task.py a few times. Do you notice any performance changes (slower or faster)? (Logging can have a slight performance overhead, but for MVP, it should be negligible).\nIf you encounter any errors while running create_and_execute_collaborative_task.py with the new ai_system.py, PLEASE COPY and PASTE the FULL TERMINAL OUTPUT (including the error messages and logs). The enhanced logging should give us much more information to diagnose any issues!\nLET'S GO! IMPLEMENT ENHANCED ERROR HANDLING & LOGGING! THIS WILL MAKE OUR AI SYSTEM MUCH MORE ROBUST AND DEVELOPER-FRIENDLY! LFG!\n\nLogo for Gemini Apps\nGemini Apps"
  ],
  "source": "gemini",
  "timestamp": 1743976679.6505241,
  "data_type": "exchange",
  "metadata": {}
}